% ============================================================================
% INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

Post-training quantization reduces model memory requirements by representing weights with fewer bits. This work systematically evaluates quantization configurations for Llama 3.2-1B on the CoQA benchmark, comparing 4-bit formats (NF4 and FP4) against the FP16 baseline.

Our goals are: (1) minimize bit-width while maximizing accuracy; (2) characterize the accuracy-memory tradeoff; (3) provide deployment recommendations for memory-constrained environments.

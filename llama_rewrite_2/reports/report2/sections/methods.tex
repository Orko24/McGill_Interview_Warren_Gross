% ============================================================================
% EXPERIMENTAL SETUP
% ============================================================================
\section{Experimental Setup}
\label{sec:methods}

\subsection{Model and Dataset}

We evaluate Llama 3.2-1B \cite{llama32}, a 1.24B parameter decoder-only transformer. For evaluation, we use the CoQA benchmark \cite{reddy2019coqa}, a conversational question answering task requiring multi-turn reasoning over passages.

\subsection{Quantization Configurations}

We compare three configurations:

\textbf{FP16 Baseline}: Standard 16-bit floating-point weights, requiring ~2.4 GB memory for the 1.24B parameter model.

\textbf{4-bit NF4}: Normal Float 4-bit with 16 non-uniformly distributed quantization levels, concentrated near zero where neural network weights are dense \cite{dettmers2023qlora}.

\textbf{4-bit FP4}: Standard 4-bit floating-point with uniformly-spaced quantization levels.

Both 4-bit formats use double quantization (quantizing the scales) and FP16 compute dtype.

\subsection{Evaluation Protocol}

We use lm-evaluation-harness \cite{eval2023harness} for zero-shot evaluation on CoQA, reporting F1 score (word-level overlap) as the primary metric and Exact Match (EM) as secondary. We evaluate on 50 samples for computational efficiency.

\subsection{Hardware}

All experiments run on NVIDIA A100-SXM4-40GB GPUs via Modal. We enable SDPA (Scaled Dot-Product Attention) and TF32 precision for efficient matrix operations.

\subsection{Metrics}

We measure:
\begin{itemize}
    \item \textbf{Accuracy}: CoQA F1 and Exact Match scores
    \item \textbf{Model size}: GPU memory for model weights
    \item \textbf{Peak memory}: Maximum GPU memory during inference
    \item \textbf{Throughput}: Tokens generated per second (batch size 8)
    \item \textbf{Latency}: Milliseconds per token during decoding
\end{itemize}

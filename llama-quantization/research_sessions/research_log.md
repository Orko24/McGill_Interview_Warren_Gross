# Llama 3.2-1B Quantization â€” Research Log

**Project**: Warren Gross Interview (McGill)  
**Goal**: Minimize bit-width while maximizing CoQA accuracy  
**Started**: January 16, 2026

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PRIOR CONTEXT (from cursor_context.md conversation)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## Initial Discussion (05:45 - 06:40)

### Assignment Discovery
- **05:45**: Received take-home assignment image
- Task: Quantize Llama 3.2-1B, minimize bit-width, maximize CoQA accuracy
- Deliverables: 4-page report + modular code
- Interviewer: Warren Gross (McGill) â€” hardware efficiency expert

### GPU Requirements
- Assignment says "targeted for GPU"
- Options discussed: Google Colab (T4), RunPod, Lambda Labs, Modal
- Llama 3.2-1B is small (~1B params) â€” fits on consumer GPUs

### Key Quantization Concepts
- **PTQ** (post-training quantization) vs **QAT** (quantization-aware training)
- **BitsAndBytes**: NF4/FP4, double quantization, no calibration needed
- **GPTQ**: Calibration-based, group_size controls accuracy
- **AWQ**: Activation-aware, protects salient weights

### Tools Identified
- `transformers` for model loading
- `bitsandbytes` / `auto-gptq` / `autoawq` for quantization
- `lm-evaluation-harness` for CoQA benchmarking

### Development Environment Decision
- User chose Cursor + Claude workflow over Jupyter notebooks
- Modal recommended for serverless GPU (pay-per-second)
- Alternative: RunPod/Lambda Labs SSH

### Timeline Discussion (06:09 - 06:11)
- Deadline: Monday
- Estimated time: 15-20 hours (1-2 days focused work)
- User decided to "hammer it out by 2pm today"

### Codebase Created (06:11)
- Complete modular codebase provided:
  - config.py, quantize.py, evaluate.py, benchmark.py
  - main.py, sweep.py, visualize.py
  - requirements.txt

### Git Setup Issues (06:40 - 06:41)
- Empty repo created at ~/Desktop/McGIll_interviews
- Fixed with proper README and code upload

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SESSION 1 CONTINUED: Docker Setup & Architecture
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## Quantization Theory Summary

**Why quantization works**: Trained weights follow normal distributions. Most values cluster near zero. You don't need 16 bits to represent "approximately 0.023".

```
FP16:  -0.0234375 (exact, 16 bits)
INT8:  -0.023     (close enough, 8 bits)  
INT4:  -0.02      (still works, 4 bits)
```

**Typical tradeoffs**:
| Bits | Size Reduction | Accuracy Loss |
|------|----------------|---------------|
| 8-bit | 2x | ~0-1% |
| 4-bit | 4x | ~2-5% |
| 3-bit | 5.3x | ~5-15% |
| 2-bit | 8x | usually broken |

**What Warren Gross cares about**:
1. Memory bandwidth â€” smaller weights = faster inference
2. Batch size â€” smaller model = more samples in VRAM
3. Integer ops â€” INT4/INT8 multiply-accumulate faster than FP16

## Docker Implementation

User requested proper Dockerfile and docker-compose.yaml (not just Modal abstraction).

Files created:
- `Dockerfile` â€” 7-layer multi-stage build
- `docker-compose.yaml` â€” GPU support, volume mounts, service definitions
- `run.sh` â€” Convenience wrapper
- `env.example` â€” HF_TOKEN template
- `.gitignore` â€” Excludes results, models, cache

---

## ðŸ“‹ Assignment Summary

| Item | Details |
|------|---------|
| Model | Llama 3.2-1B (meta-llama/Llama-3.2-1B) |
| Benchmark | CoQA (Conversational QA) via lm-evaluation-harness |
| Deliverables | 4-page report + reproducible code |
| Focus | Hardware efficiency (Warren Gross's specialty) |

---

## ðŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              HOST MACHINE                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  llama-quantization/                                                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ .env                 â† HF_TOKEN goes here                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ docker-compose.yaml  â† Orchestrates everything                 â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Dockerfile           â† 7-layer build                           â”‚   â”‚
â”‚  â”‚  â””â”€â”€ results/             â† Output (volume-mapped)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â”‚ docker-compose run --rm quick          â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        DOCKER CONTAINER                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  /app/ (working directory)                                     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ config.py      â† Experiment definitions                  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ quantize.py    â† Model loading + quantization            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ evaluate.py    â† CoQA benchmarking                       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ benchmark.py   â† Hardware profiling                      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ main.py        â† Single experiment runner                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ sweep.py       â† Multi-experiment orchestration          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€ visualize.py   â† Figure generation                       â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                              â”‚                                       â”‚   â”‚
â”‚  â”‚                              â–¼                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  /cache/huggingface/ (Docker Volume: hf-cache)                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€ models--meta-llama--Llama-3.2-1B/  (~4GB, cached)        â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                              â”‚                                       â”‚   â”‚
â”‚  â”‚                              â–¼                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  NVIDIA GPU (via nvidia-docker)                                â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€ CUDA 12.1 + cuDNN 8                                      â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ./results/ (volume-mapped back to host)                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ fp16_baseline_results.json                                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ bnb_8bit_results.json                                          â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ bnb_4bit_nf4_results.json                                      â”‚   â”‚
â”‚  â”‚  â””â”€â”€ figures/                                                        â”‚   â”‚
â”‚  â”‚      â”œâ”€â”€ accuracy_vs_compression.png                                â”‚   â”‚
â”‚  â”‚      â””â”€â”€ memory_comparison.png                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ³ Docker Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 7: Application Code                              (~50KB) â”‚
â”‚  COPY *.py /app/                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 6: Visualization                                (~300MB) â”‚
â”‚  pip install numpy pandas matplotlib seaborn                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 5: Evaluation Framework                         (~200MB) â”‚
â”‚  pip install lm-eval>=0.4.0                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 4: Quantization Libraries                       (~100MB) â”‚
â”‚  pip install bitsandbytes>=0.41.0                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: ML Dependencies                              (~500MB) â”‚
â”‚  pip install transformers accelerate datasets                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: PyTorch + CUDA                                (~5GB)  â”‚
â”‚  pip install torch==2.2.0 (cu121)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: Base Image                                    (~3GB)  â”‚
â”‚  nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: ~9GB (but layers 1-5 are cached after first build)
Rebuild time after code change: <10 seconds
```

---

## ðŸ“¦ Code Module Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                config.py                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ QuantMethod     â”‚  â”‚ QuantConfig     â”‚  â”‚ ExperimentConfigâ”‚             â”‚
â”‚  â”‚ (Enum)          â”‚  â”‚ (dataclass)     â”‚  â”‚ (dataclass)     â”‚             â”‚
â”‚  â”‚ - NONE          â”‚  â”‚ - method        â”‚  â”‚ - name          â”‚             â”‚
â”‚  â”‚ - BNB_8BIT      â”‚  â”‚ - bnb_4bit_*    â”‚  â”‚ - quantization  â”‚             â”‚
â”‚  â”‚ - BNB_4BIT      â”‚  â”‚ - gptq_*        â”‚  â”‚ - model         â”‚             â”‚
â”‚  â”‚ - GPTQ          â”‚  â”‚ - awq_*         â”‚  â”‚ - eval          â”‚             â”‚
â”‚  â”‚ - AWQ           â”‚  â”‚                 â”‚  â”‚ - benchmark     â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                    â”‚                    â”‚                       â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                â”‚                                            â”‚
â”‚                    EXPERIMENTS dict (predefined configs)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               quantize.py                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ load_quantized_model(config) â†’ (model, tokenizer)                    â”‚   â”‚
â”‚  â”‚     â”‚                                                                â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ load_model_fp16()      # Baseline, no quantization          â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ load_model_bnb()       # BitsAndBytes 4/8-bit               â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ load_model_gptq()      # GPTQ with calibration              â”‚   â”‚
â”‚  â”‚     â””â”€â”€ load_model_awq()       # AWQ with calibration               â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚ get_model_memory_footprint(model) â†’ dict                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               evaluate.py                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ evaluate_model(model, tokenizer, config) â†’ dict                      â”‚   â”‚
â”‚  â”‚     â”‚                                                                â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ run_quick_sanity_check()  # Verify model generates text     â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ run_lm_eval()             # lm-evaluation-harness wrapper   â”‚   â”‚
â”‚  â”‚     â””â”€â”€ extract_coqa_metrics()    # Pull F1 and EM scores           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               benchmark.py                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ run_benchmarks(model, tokenizer, config) â†’ dict                      â”‚   â”‚
â”‚  â”‚     â”‚                                                                â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ measure_model_size()          # Params * bytes              â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ measure_prefill_latency()     # Time to process prompt      â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ measure_decode_latency()      # Time per generated token    â”‚   â”‚
â”‚  â”‚     â””â”€â”€ measure_throughput()          # Tokens/sec at batch sizes   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                main.py                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ run_experiment(config) â†’ dict                                        â”‚   â”‚
â”‚  â”‚     â”‚                                                                â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ 1. load_quantized_model()                                   â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ 2. evaluate_model()                                         â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ 3. run_benchmarks()                                         â”‚   â”‚
â”‚  â”‚     â””â”€â”€ 4. save results to JSON                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                sweep.py                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ run_sweep(configs, limit) â†’ summary                                  â”‚   â”‚
â”‚  â”‚     â”‚                                                                â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ generate_bnb_configs()   # All BnB variations               â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ generate_gptq_configs()  # Strategic GPTQ configs           â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€ generate_awq_configs()   # AWQ variations                   â”‚   â”‚
â”‚  â”‚     â”‚                                                                â”‚   â”‚
â”‚  â”‚     â””â”€â”€ for each config:                                            â”‚   â”‚
â”‚  â”‚             run_experiment(config)                                   â”‚   â”‚
â”‚  â”‚             save intermediate results                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              visualize.py                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Input: results/*.json files                                          â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚ Output:                                                              â”‚   â”‚
â”‚  â”‚   â”œâ”€â”€ accuracy_vs_compression.png   # Pareto frontier               â”‚   â”‚
â”‚  â”‚   â”œâ”€â”€ memory_comparison.png         # Bar chart                     â”‚   â”‚
â”‚  â”‚   â”œâ”€â”€ latency_comparison.png        # Speed metrics                 â”‚   â”‚
â”‚  â”‚   â””â”€â”€ summary_table.png             # For report                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Data Flow

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  HuggingFace â”‚
                    â”‚     Hub      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ Download (first run only)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    quantize.py                                â”‚
â”‚  Llama 3.2-1B (FP16) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
â”‚       â”‚                                                       â”‚
â”‚       â”œâ”€â”€ BitsAndBytes â”€â”€â–º 8-bit model                       â”‚
â”‚       â”œâ”€â”€ BitsAndBytes â”€â”€â–º 4-bit NF4 model                   â”‚
â”‚       â”œâ”€â”€ BitsAndBytes â”€â”€â–º 4-bit FP4 model                   â”‚
â”‚       â”œâ”€â”€ GPTQ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 4-bit calibrated model            â”‚
â”‚       â””â”€â”€ AWQ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 4-bit activation-aware model      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    evaluate.py                                â”‚
â”‚                                                               â”‚
â”‚  Model + CoQA Dataset â”€â”€â–º lm-evaluation-harness â”€â”€â–º F1, EM   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    benchmark.py                               â”‚
â”‚                                                               â”‚
â”‚  Model â”€â”€â–º Memory profiling    â”€â”€â–º peak_mb, allocated_mb     â”‚
â”‚        â”€â”€â–º Latency profiling   â”€â”€â–º ms/token                  â”‚
â”‚        â”€â”€â–º Throughput profiling â”€â”€â–º tokens/sec               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    results/*.json                             â”‚
â”‚                                                               â”‚
â”‚  {                                                            â”‚
â”‚    "experiment_name": "bnb_4bit_nf4",                        â”‚
â”‚    "evaluation": { "coqa_f1": 0.723, "coqa_em": 0.451 },     â”‚
â”‚    "benchmarks": { "memory_peak_mb": 612, "tps": 45.2 }      â”‚
â”‚  }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    visualize.py                               â”‚
â”‚                                                               â”‚
â”‚  JSON files â”€â”€â–º matplotlib â”€â”€â–º Publication-ready figures     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ File Manifest

| File | Lines | Purpose | Depends On |
|------|-------|---------|------------|
| `config.py` | 174 | Experiment definitions | - |
| `quantize.py` | 369 | Model loading + quantization | config |
| `evaluate.py` | 270 | CoQA evaluation | config |
| `benchmark.py` | 391 | Hardware profiling | config |
| `main.py` | 286 | Single experiment | quantize, evaluate, benchmark |
| `sweep.py` | 380 | Multi-experiment | main, config |
| `visualize.py` | 456 | Figure generation | - |
| `Dockerfile` | 80 | Container build | - |
| `docker-compose.yaml` | 120 | Orchestration | Dockerfile |

---

## ðŸš€ Execution Commands

```bash
# Build (once)
docker-compose build

# Quick comparison (minimum viable submission)
docker-compose run --rm quick

# Full pipeline (all experiments + figures)
docker-compose run --rm full-run

# Individual experiments
docker-compose run --rm baseline
docker-compose run --rm bnb-8bit
docker-compose run --rm bnb-4bit

# Custom experiment
docker-compose run --rm quant python main.py --experiment gptq_4bit_g128 --limit 100

# Generate figures
docker-compose run --rm figures

# Interactive shell (for debugging)
docker-compose run --rm quant
```

---

## ðŸ“Š Expected Results

| Experiment | Bits | Size (MB) | CoQA F1 | Memory (MB) |
|------------|------|-----------|---------|-------------|
| fp16_baseline | 16 | ~2000 | ~0.76 | ~2500 |
| bnb_8bit | 8 | ~1000 | ~0.75 | ~1300 |
| bnb_4bit_nf4 | 4 | ~600 | ~0.72 | ~800 |
| bnb_4bit_fp4 | 4 | ~600 | ~0.71 | ~800 |
| gptq_4bit_g128 | 4 | ~600 | ~0.73 | ~800 |

*Note: Actual results will vary. These are estimates based on typical quantization performance.*

---

## ðŸ“ Session Log

### Session 1: January 16, 2026

**Time**: 07:00 - present

**Completed**:
- [x] Understood assignment requirements
- [x] Created modular codebase structure
- [x] Implemented Docker containerization (7-layer build)
- [x] Set up docker-compose with GPU support
- [x] Created all Python modules (config, quantize, evaluate, benchmark, main, sweep, visualize)
- [x] Added .gitignore for clean repo

**Next**:
- [ ] Set up .env with HF_TOKEN
- [ ] Build Docker image
- [ ] Run quick comparison (FP16/8bit/4bit)
- [ ] Generate figures
- [ ] Write 4-page report

---

## ðŸ”— References

- [Llama 3.2-1B on HuggingFace](https://huggingface.co/meta-llama/Llama-3.2-1B)
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)
- [CoQA Dataset](https://stanfordnlp.github.io/coqa/)

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# APPEND NEW ENTRIES BELOW THIS LINE
# (Never delete above â€” log is append-only)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## Entry: January 16, 2026 â€” Keys & GPU Setup

### ðŸ”‘ Required API Keys

#### 1. HuggingFace Token (REQUIRED)
- **Why**: Llama 3.2 is a gated model. You must accept Meta's license to download.
- **Get it**: https://huggingface.co/settings/tokens
- **Steps**:
  1. Go to https://huggingface.co/meta-llama/Llama-3.2-1B
  2. Click "Access repository" and accept the license
  3. Go to https://huggingface.co/settings/tokens
  4. Create a new token (read access is sufficient)
  5. Copy the token (starts with `hf_`)

#### 2. Modal Token (if using Modal)
- **Why**: Modal needs to authenticate your account
- **Get it**: Automatically created during `modal setup`
- **Steps**:
  ```bash
  pip install modal
  modal setup  # Opens browser, logs you in, saves token locally
  ```
- **Note**: Modal stores the token in `~/.modal/credentials` â€” no need to manage it manually

#### 3. No Other Keys Needed
- Docker: No key (local)
- NVIDIA: No key (driver handles GPU access)
- lm-eval-harness: No key (public datasets)

---

### ðŸ–¥ï¸ How to Know Which GPU is Commissioned

#### Option A: Docker (local or cloud VM)

The GPU is whatever's attached to the machine running Docker.

```bash
# Check GPU before running
nvidia-smi

# Output shows:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.154.05   Driver Version: 535.154.05   CUDA Version: 12.2     |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  NVIDIA A10G         Off  | 00000000:00:1E.0 Off |                    0 |
# |  0%   30C    P0    51W / 300W |      0MiB / 23028MiB |      0%      Default |
# +-------------------------------+----------------------+----------------------+
```

Inside Docker container:
```bash
docker-compose run --rm quant python -c "import torch; print(torch.cuda.get_device_name())"
# Output: NVIDIA A10G
```

#### Option B: Modal (serverless)

You SPECIFY the GPU in your code:

```python
@app.function(gpu="A10G")  # â† You choose this
def my_function():
    pass
```

**Modal GPU options**:
| GPU | VRAM | Cost/hr | Good for |
|-----|------|---------|----------|
| `T4` | 16GB | ~$0.60 | Budget, testing |
| `A10G` | 24GB | ~$1.10 | Our experiments âœ“ |
| `A100` | 40GB | ~$3.00 | Large models |
| `A100-80GB` | 80GB | ~$4.50 | Very large models |

The `modal_app.py` I created uses `A10G` by default:
```python
@app.function(gpu="A10G", ...)  # Line 76 in modal_app.py
```

#### How to Verify GPU During Experiment

The code logs GPU info automatically:
```
============================================================
Running experiment: bnb_4bit_nf4
GPU: NVIDIA A10G          â† Shows which GPU
VRAM: 24.00 GB            â† Shows available memory
============================================================
```

---

### ðŸ“‹ Setup Checklist

```
[ ] 1. Get HuggingFace token
      â†’ https://huggingface.co/settings/tokens
      
[ ] 2. Accept Llama 3.2 license  
      â†’ https://huggingface.co/meta-llama/Llama-3.2-1B
      
[ ] 3. Add token to .env file
      â†’ cp env.example .env
      â†’ Edit: HF_TOKEN=hf_xxxxxxxxx

[ ] 4. (If using Modal) Run modal setup
      â†’ pip install modal
      â†’ modal setup

[ ] 5. Build Docker image
      â†’ docker-compose build

[ ] 6. Test GPU access
      â†’ docker-compose run --rm quant python -c "import torch; print(torch.cuda.get_device_name())"
```

### env.example Contents

```bash
# REQUIRED
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx   # Get from huggingface.co/settings/tokens

# OPTIONAL
# CUDA_VISIBLE_DEVICES=0      # If multiple GPUs
# WANDB_API_KEY=xxx           # If using Weights & Biases
```

**To use**: 
```bash
cp env.example .env
nano .env   # Replace hf_xxx with your actual token
```

---

## Entry: January 16, 2026 â€” PyTorch Optimizations Added

### âš¡ Optimizations Now Enabled

1. **SDPA / Flash Attention 2** (`quantize.py`)
   - `attn_implementation="sdpa"` on all model loads
   - Uses PyTorch's native efficient attention
   - ~2x faster attention, lower memory

2. **TF32 Precision** (`benchmark.py`)
   - `torch.backends.cuda.matmul.allow_tf32 = True`
   - ~3x faster matmuls on Ampere+ GPUs (A10G, A100)
   - Negligible accuracy loss

3. **cuDNN Benchmark Mode**
   - `torch.backends.cudnn.benchmark = True`
   - Auto-tunes convolution algorithms

4. **inference_mode()** instead of `no_grad()`
   - Slightly faster (disables version tracking)

### ðŸ“Š Progress Bars Added

- `tqdm` now shows progress for:
  - Experiment loop (`ðŸ”¬ Experiments`)
  - Prefill latency (`ðŸ“Š Prefill latency`)
  - Decode latency (`ðŸ“Š Decode latency`)
  - Throughput (`ðŸ“Š Throughput`)

---

## Entry: January 16, 2026 â€” Modal Setup Complete (~08:00)

### âœ… Modal Authentication

1. Installed pipx: `sudo apt install -y pipx && pipx ensurepath`
2. Installed modal: `pipx install modal`
3. Authenticated: `modal setup` â†’ Browser auth completed
4. Workspace: `scaleaiwork0301`
5. Credits: $5.00 available (unlockable to $30 with card)

### ðŸ”§ Remaining Setup

```bash
# Still need to do:
modal secret create huggingface HF_TOKEN=hf_xxxxx  # Add HF token to Modal
modal run modal_app.py::test_gpu                    # Verify GPU access
modal run modal_app.py --quick --limit 50          # Run experiments
```

### ðŸ’° Modal Pricing (for reference)

| GPU | VRAM | $/hr | Estimated cost for our experiments |
|-----|------|------|-----------------------------------|
| T4 | 16GB | $0.60 | ~$0.30 |
| A10G | 24GB | $1.10 | ~$0.55 (using this) |
| A100 | 40GB | $3.00 | ~$1.50 |

---

## Entry: January 16, 2026 â€” Strategy Decision (~08:15)

### ðŸŽ¯ Execution Order (IMPORTANT)

**DO THIS:**
1. âœ… Run experiments first â†’ Get JSON results
2. âœ… Generate figures â†’ Visualize results  
3. âœ… Write report â†’ Main deliverable
4. â³ Refactor code â†’ Only if time permits

**WHY:**
- Report needs data. Can't write without results.
- Ugly working code > pretty broken code
- Interview tests understanding, not code style
- Time is limited (deadline: Monday)

### ðŸ› Known AI Code Quirks (acknowledged)

Current code has typical AI-generated patterns:
- Excessive standalone functions (not OOP)
- Some imports inside functions
- Verbose logging
- Repetitive patterns

**Plan**: Run first, refactor later if time. The results matter more than code aesthetics for this submission.

---

## Entry: January 16, 2026 â€” Full Timeline Summary

### ðŸ“… Session Timeline

| Time | Event |
|------|-------|
| ~06:00 | Received assignment email |
| 05:45-06:40 | Initial planning, understood requirements |
| 06:40-07:00 | Created modular codebase (7 Python files) |
| 07:00-07:30 | Docker setup (Dockerfile, docker-compose) |
| 07:30-08:00 | Modal setup + authentication |
| 08:00-08:15 | Added PyTorch optimizations + progress bars |
| 08:15 | Taking break |

### ðŸ“¦ Files Created (18 total)

**Python Modules:**
- `config.py` (174 lines) â€” Experiment configurations
- `quantize.py` (382 lines) â€” Model loading + quantization
- `evaluate.py` (270 lines) â€” CoQA evaluation
- `benchmark.py` (415 lines) â€” Hardware profiling
- `main.py` (287 lines) â€” Single experiment runner
- `sweep.py` (380 lines) â€” Multi-experiment orchestration
- `visualize.py` (456 lines) â€” Figure generation
- `modal_app.py` (398 lines) â€” Modal serverless integration

**Infrastructure:**
- `Dockerfile` â€” 7-layer container build
- `docker-compose.yaml` â€” GPU orchestration
- `run.sh` â€” Convenience wrapper
- `requirements.txt` â€” Dependencies
- `env.example` â€” Environment template
- `.gitignore` â€” Exclusions
- `README.md` â€” Documentation

**Research:**
- `research_sessions/research_log.md` â€” This file

### ðŸš¦ Current Status

```
[âœ…] Assignment understood
[âœ…] Codebase created
[âœ…] Docker setup complete
[âœ…] Modal authenticated
[âœ…] PyTorch optimizations added
[âœ…] Progress bars added
[â³] HuggingFace token â†’ Need to add to Modal secrets
[â³] Run experiments â†’ ~30 min GPU time
[â³] Generate figures â†’ ~5 min
[â³] Write report â†’ ~2-3 hours
[â³] (Optional) Refactor code
```

### â±ï¸ Estimated Time Remaining

| Task | Time |
|------|------|
| Add HF token to Modal | 2 min |
| Run experiments | 30-45 min (GPU) |
| Generate figures | 5 min |
| Write 4-page report | 2-3 hours |
| (Optional) Refactor | 1-2 hours |
| **Total** | **3-5 hours** |

---

## ðŸ“š Interview Prep â€” Key Concepts to Know

### Quantization Methods

**BitsAndBytes (BnB)**
- On-the-fly quantization during model load
- No calibration data needed
- 8-bit: Linear.int8() with outlier handling
- 4-bit: NF4 (normal float 4) or FP4
- NF4 > FP4 for accuracy (optimized for normal distributions)

**GPTQ**
- Post-training quantization with calibration
- Uses Hessian information to minimize quantization error
- `group_size` parameter: smaller = better accuracy, larger = faster
- Typical: group_size=128

**AWQ (Activation-Aware)**
- Identifies "salient weights" that affect activations most
- Protects important weights from aggressive quantization
- Often better accuracy than GPTQ at same bit-width
- 4-bit only

### Hardware Efficiency (Warren Gross focus)

**Why quantization helps hardware:**
1. **Memory bandwidth**: 4-bit weights = 4x less data to move
2. **Cache utilization**: Smaller model fits in GPU cache
3. **Batch size**: Less VRAM per model = more samples in parallel
4. **Integer ops**: INT4/INT8 faster than FP16 on tensor cores

**Metrics that matter:**
- Model size (MB)
- Peak memory during inference (MB)
- Prefill latency (ms) â€” time to process prompt
- Decode latency (ms/token) â€” time per generated token
- Throughput (tokens/sec) â€” at various batch sizes

### CoQA Evaluation

- Conversational QA benchmark
- Metrics: F1 score, Exact Match (EM)
- Tests multi-turn dialogue understanding
- Running subset (50-100 samples) is valid for comparison

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SESSION 2: Full Experiment Execution (January 16, 2026 - Afternoon)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## 13:00 - Modal Execution & Debugging

### Initial Runs
- First successful Modal run with FP16 baseline
- **FP16 Results**: F1 = 62.48%, Memory = 2357 MB

### 8-bit BitsAndBytes Bug
**Error encountered:**
```
Error invalid configuration argument at line 380 in file /src/csrc/ops.cu
```

**Root cause**: BitsAndBytes LLM.int8() CUDA kernel incompatible with A10G GPU architecture

**Attempted fixes:**
1. Added `llm_int8_threshold=6.0`
2. Set `llm_int8_has_fp16_weight=False`
3. Changed Modal base image to `nvidia/cuda:12.1.0-devel`
4. Pinned `bitsandbytes>=0.43.0`

**Resolution**: Skipped 8-bit quantization, focused on 4-bit (more interesting compression anyway)

### Deserialization Bug
**Error:**
```
DeserializationError: Deserialization failed because the 'torch' module is not available in the local environment.
```

**Cause**: Modal tries to return torch tensors to local machine which doesn't have torch installed

**Fix**: 
1. Installed torch locally in venv
2. Modified code to return JSON-serializable dictionaries instead

---

## 13:30 - Quick Comparison Complete

### Results (50 samples)
| Config | F1 Score | Memory |
|--------|----------|--------|
| FP16 Baseline | 62.48% | 2357 MB |
| BnB 4-bit NF4 | 67.58% | 965 MB |

### Key Finding
**NF4 achieved 2.44x compression with NO accuracy loss** (actually slightly better, likely sampling noise)

---

## 14:00 - Extended Comparison (results2.json)

### Experiments Run
1. FP16 Baseline
2. BnB 4-bit NF4
3. BnB 4-bit FP4
4. QLoRA memory estimation

### Results
| Config | F1 Score | Memory |
|--------|----------|--------|
| FP16 Baseline | 62.48% | 2357 MB |
| BnB 4-bit NF4 | **67.58%** | 965 MB |
| BnB 4-bit FP4 | 56.28% | 965 MB |
| QLoRA Ready | N/A | 965 MB + 29 MB LoRA |

### Critical Finding
**NF4 >> FP4**: +11.3% F1 improvement at identical memory cost!

---

## 14:30 - Hyperparameter Ablation (results3.json)

### Ablation Factors Tested
1. **Quantization type**: NF4 vs FP4
2. **Double quantization**: On vs Off
3. **Compute dtype**: FP16 vs BF16

### Full Results Table
| Experiment | Quant Type | Double Q | Dtype | F1 | Memory |
|------------|------------|----------|-------|-----|--------|
| fp16_baseline | - | - | - | 64.18% | 2357 MB |
| bnb_4bit_nf4 | NF4 | Yes | FP16 | **67.58%** | 965 MB |
| bnb_4bit_nf4_no_double | NF4 | No | FP16 | 67.58% | 965 MB |
| bnb_4bit_nf4_bf16 | NF4 | Yes | BF16 | 67.58% | 965 MB |
| bnb_4bit_fp4 | FP4 | Yes | FP16 | 58.07% | 965 MB |
| bnb_4bit_fp4_no_double | FP4 | No | FP16 | 58.86% | 965 MB |

### Key Findings from Ablation

1. **Quant Type Impact**: NF4 is dramatically better than FP4
   - NF4: 67.58% F1
   - FP4: ~58% F1
   - **Delta: +9.5% absolute, +16% relative**

2. **Double Quantization Impact**: NONE
   - With double quant: 67.58%
   - Without double quant: 67.58%
   - **Conclusion: Enable double quant for free compression**

3. **Compute Dtype Impact**: NONE
   - FP16: 67.58%
   - BF16: 67.58%
   - **Conclusion: Use whatever hardware prefers**

---

## Bugs Fixed This Session

### Bug 1: F-string Formatting Error
```python
# BROKEN:
print(f"{f1:.4f if isinstance(f1, float) else f1}")

# FIXED:
f1_str = f"{f1:.4f}" if isinstance(f1, (int, float)) else str(f1)
print(f"{f1_str}")
```

### Bug 2: Right-Padding Warning
```
A decoder-only architecture is being used, but right-padding was detected!
```
**Fix**: Added `padding_side="left"` to tokenizer loading

### Bug 3: torch_dtype Deprecation
```
`torch_dtype` is deprecated! Use `dtype` instead!
```
**Fix**: Changed `torch_dtype=` to `dtype=` in model loading

### Bug 4: TRANSFORMERS_CACHE Warning
```
Using `TRANSFORMERS_CACHE` is deprecated... Use `HF_HOME` instead.
```
**Fix**: Added warning filter to suppress (not critical)

### Bug 5: NumPy 2.x Incompatibility
**Fix**: Pinned `numpy<2.0.0` in Modal image

---

## Code Improvements Made

### 1. Fail-Fast Logic
Stop on first error to save compute credits:
```python
except Exception as e:
    print(f"ðŸ›‘ FAIL-FAST: Stopping to save credits")
    return partial_results
```

### 2. Incremental Saves
Save after each experiment:
```python
for exp in experiments:
    result = run(exp)
    all_results.append(result)
    save_to_json(all_results)  # Save immediately
    print(f"ðŸ’¾ Saved {len(all_results)}/{len(experiments)}")
```

### 3. Progress Bars (tqdm)
Added visual feedback for long operations:
```python
for i in tqdm(range(20), desc="ðŸ“Š Decode latency"):
    measure_decode()
```

### 4. PyTorch Optimizations
```python
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.benchmark = True
# Use inference_mode() instead of no_grad()
with torch.inference_mode():
    output = model.generate(...)
```

---

## Final Results Summary

### Best Configuration
**BitsAndBytes 4-bit NF4 with double quantization enabled**

| Metric | FP16 Baseline | Best (NF4) | Improvement |
|--------|---------------|------------|-------------|
| F1 Score | 64.18% | 67.58% | +3.4% |
| Memory | 2357 MB | 965 MB | **59% reduction** |
| Compression | 1.0x | 2.44x | **2.44x smaller** |

### What We Learned
1. **NF4 >> FP4** at same memory (9.5% better)
2. **Double quant = free** (no accuracy cost)
3. **FP16 â‰ˆ BF16** for compute dtype
4. **4-bit beats FP16** on this benchmark (likely noise, but proves no degradation)

---

## Files Created This Session

### Results
- `results/quick_comparison.json` - FP16 vs NF4
- `results/results2.json` - Extended comparison (66K lines with all eval data)
- `results/results2_summary.json` - Clean summary
- `results/results3.json` - Hyperparameter ablation
- `results/results3_summary.json` - Clean summary

### Documentation
- `design_report/01_GLOSSARY.md` - All terms explained
- `design_report/02_ARCHITECTURE.md` - System diagrams
- `design_report/03_DESIGN_CHOICES.md` - Why we did what we did

### Code Updates
- `config.py` - Added ablation experiment configs
- `modal_app.py` - Added --hyperparam CLI, incremental saves, fail-fast
- `quantize.py` - Fixed 8-bit config, added padding_side
- `benchmark.py` - Added PyTorch optimizations, warning filters

---

## Total Compute Cost
- **Modal credits used**: ~$3-4
- **Remaining**: ~$1

---

## Next Steps (for report)
1. Run full evaluation (no limit) for publication-quality numbers
2. Generate visualizations (accuracy vs compression curves)
3. Write 4-page report following scientific paper format
4. Clean up code for submission

---

# Session 3: LaTeX Report Setup
**Date**: January 16, 2026
**Time**: ~3:30 PM

---

## What We Did

### Created LaTeX Report Structure

Set up modular LaTeX report in `reports/report1/`:

```
reports/report1/
â”œâ”€â”€ main.tex              # Master document (imports sections)
â”œâ”€â”€ references.bib        # BibTeX bibliography
â”œâ”€â”€ Makefile              # Build automation
â”œâ”€â”€ sections/
â”‚   â”œâ”€â”€ abstract.tex      # ~100 word summary
â”‚   â”œâ”€â”€ introduction.tex  # Problem + contributions
â”‚   â”œâ”€â”€ methods.tex       # Quantization techniques + setup
â”‚   â”œâ”€â”€ results.tex       # Tables with all findings
â”‚   â”œâ”€â”€ discussion.tex    # Analysis + recommendations
â”‚   â””â”€â”€ conclusion.tex    # Summary + reproducibility
â””â”€â”€ figures/              # For diagrams (empty for now)
```

### Key Design Decisions

1. **Modular sections**: Each section in its own file â†’ easy editing
2. **BibTeX**: Separate `.bib` file â†’ clean reference management
3. **Makefile**: `make` builds full PDF, `make quick` for drafts
4. **Two-column format**: Standard scientific paper layout
5. **10pt font**: Fits 4 pages comfortably

### Report Content (Draft)

**Abstract highlights**:
- 4-bit NF4 achieves 2.44Ã— compression with no accuracy loss
- NF4 outperforms FP4 by 9.5% F1
- Double quantization is "free"

**Tables prepared**:
1. Main results (NF4 vs FP4 vs baseline)
2. Double quantization ablation
3. Compute dtype ablation
4. Hardware performance (placeholder)

**References added** (10 key papers):
- Llama original paper
- BitsAndBytes (LLM.int8())
- QLoRA paper
- CoQA dataset
- GPTQ, AWQ papers
- Quantization survey

---

## To Compile the Report

```bash
cd ~/Desktop/McGIll_interviews/llama-quantization/reports/report1
make        # Full build with bibliography
make quick  # Quick build (no bib)
```

Requires: `pdflatex` and `bibtex` (install with `sudo apt install texlive-latex-base texlive-bibtex-extra`)

---

## Project Structure Update (Complete)

```
llama-quantization/
â”œâ”€â”€ ðŸ“ Core Code
â”‚   â”œâ”€â”€ config.py, quantize.py, evaluate.py
â”‚   â”œâ”€â”€ benchmark.py, main.py, sweep.py, visualize.py
â”‚   â””â”€â”€ modal_app.py         # Main entry point
â”‚
â”œâ”€â”€ ðŸ“ Results
â”‚   â””â”€â”€ results/*.json       # All experiment data
â”‚
â”œâ”€â”€ ðŸ“ Documentation
â”‚   â”œâ”€â”€ design_report/       # Technical docs
â”‚   â””â”€â”€ research_sessions/   # This log
â”‚
â”œâ”€â”€ ðŸ“ Reports (NEW)
â”‚   â””â”€â”€ report1/             # LaTeX report v1
â”‚       â”œâ”€â”€ main.tex
â”‚       â”œâ”€â”€ sections/*.tex
â”‚       â””â”€â”€ references.bib
â”‚
â”œâ”€â”€ ðŸ“ Config
â”‚   â”œâ”€â”€ .env, env.example
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ ðŸ“ Legacy (kept for showcase)
    â”œâ”€â”€ Dockerfile, docker-compose.yaml
    â””â”€â”€ run.sh
```

---

## Session 3b: Visualizations Added
**Time**: ~4:00 PM

### Generated 6 Publication Figures

Created `reports/report1/generate_figures.py` - generates all figures automatically:

| Figure | File | Description |
|--------|------|-------------|
| Fig 1 | `fig1_accuracy_vs_memory.pdf` | Scatter plot: accuracy vs memory tradeoff |
| Fig 2 | `fig2_all_configurations.pdf` | Bar chart: all 6 configurations compared |
| Fig 3 | `fig3_nf4_vs_fp4.pdf` | **Key figure**: NF4 vs FP4 side-by-side |
| Fig 4 | `fig4_ablation_heatmap.pdf` | Heatmap: quant type Ã— double quant |
| Fig 5 | `fig5_compression_waterfall.pdf` | Waterfall: memory reduction breakdown |
| Fig 6 | `fig6_summary_infographic.pdf` | Key metrics at a glance |

### To Regenerate Figures
```bash
cd ~/Desktop/McGIll_interviews/llama-quantization
source .venv/bin/activate
python reports/report1/generate_figures.py
```

### Report Compilation Status
- **Current**: 5 pages (needs trimming to 4)
- **Figures**: âœ“ All 6 included in results section
- **Bibliography**: âœ“ 10 citations compiled

### To Compile Report
```bash
cd ~/Desktop/McGIll_interviews/llama-quantization/reports/report1
make  # Full build with bibtex
```

---


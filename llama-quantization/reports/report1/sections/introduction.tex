% ============================================================================
% INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:introduction}

Large Language Models (LLMs) have demonstrated remarkable capabilities across natural language tasks, but their deployment is constrained by substantial memory and computational requirements \cite{touvron2023llama}. A 1-billion parameter model in FP16 precision requires approximately 2.4 GB of GPU memory for weights alone, limiting deployment on edge devices and increasing inference costs in cloud environments.

\textbf{Quantization}---reducing the numerical precision of model weights---offers a promising solution. By representing weights with fewer bits (e.g., 4-bit instead of 16-bit), we can achieve significant memory reduction. However, aggressive quantization risks accuracy degradation, necessitating careful study of the accuracy-compression trade-off.

\subsection{Contributions}

This work makes the following contributions:

\begin{enumerate}
    \item \textbf{Systematic comparison} of quantization formats (NF4 vs. FP4) on Llama 3.2-1B, finding NF4 superior by 9.5\% F1.
    
    \item \textbf{Hyperparameter ablation} studying double quantization and compute dtype effects, finding double quantization provides free compression.
    
    \item \textbf{Reproducible codebase} with modular design, enabling easy experimentation with different quantization configurations.
\end{enumerate}

\subsection{Problem Statement}

Given the pre-trained Llama 3.2-1B model, our objective is to:
\begin{equation}
    \min_{\theta_q} \text{BitWidth}(\theta_q) \quad \text{s.t.} \quad \text{F1}(\theta_q) \geq \text{F1}(\theta) - \epsilon
\end{equation}
where $\theta$ represents the original FP16 weights, $\theta_q$ the quantized weights, and $\epsilon$ an acceptable accuracy drop threshold.


\begin{thebibliography}{1}

\bibitem{dettmers2022llmint8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock {\em Advances in Neural Information Processing Systems},
  35:30318--30332, 2022.

\bibitem{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{eval2023harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
  DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, et~al.
\newblock A framework for few-shot language model evaluation.
\newblock In {\em Zenodo}, 2023.

\bibitem{llama32}
{Meta AI}.
\newblock Llama 3.2: Lightweight models for edge devices.
\newblock
  \url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
  2024.

\bibitem{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D Manning.
\newblock Coqa: A conversational question answering challenge.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:249--266, 2019.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\end{thebibliography}

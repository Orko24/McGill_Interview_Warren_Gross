% ============================================================================
% CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We systematically evaluated post-training quantization for Llama 3.2-1B on CoQA. Our experiments yield three actionable findings:

\begin{enumerate}
    \item 4-bit NF4 quantization achieves 2.44$\times$ memory compression with no accuracy degradation, and potentially slight improvement over FP16.
    
    \item Quantization format selection is critical: NF4 outperforms FP4 by 9.5\% F1 at identical memory cost, validating distribution-aware quantization.
    
    \item Double quantization provides free additional compression without accuracy penalty.
\end{enumerate}

These findings support deploying Llama 3.2-1B at 4-bit NF4 precision for memory-constrained environments, reducing GPU memory from 2.4 GB to under 1 GB while preserving task accuracy.

\subsection*{Reproducibility}

Code and configurations available at the submission repository. To reproduce:
\begin{lstlisting}[language=bash]
modal run modal_app.py --hyperparam
\end{lstlisting}

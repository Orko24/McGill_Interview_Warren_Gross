{
  "experiments": [
    {
      "experiment_name": "fp16_baseline",
      "status": "success",
      "model_size_mb": 2357.1290283203125,
      "coqa_metrics": {
        "coqa_f1": 0.6247836607836608,
        "coqa_em": 0.52,
        "coqa_f1_stderr": 0.05925425674333541,
        "coqa_em_stderr": 0.06643497152988682
      },
      "benchmarks": {
        "model_size_mb": 2357.1290283203125,
        "memory_allocated_mb": 2384.46435546875,
        "memory_peak_mb": 2384.72216796875,
        "prefill_latency_ms": {
          "128": 13.422407799998837,
          "256": 13.190483200000358,
          "512": 13.215660000003027,
          "1024": 14.94928579999737
        },
        "decode_latency_ms_per_token": 12.876591693749992,
        "throughput_tokens_per_sec": {
          "1": 77.89627813937996,
          "4": 195.74844110492023,
          "8": 381.34730939426424
        },
        "device": "NVIDIA A100-SXM4-40GB"
      }
    },
    {
      "experiment_name": "bnb_4bit_nf4",
      "status": "success",
      "model_size_mb": 965.1290283203125,
      "coqa_metrics": {
        "coqa_f1": 0.6758413830178535,
        "coqa_em": 0.5783333333333334,
        "coqa_f1_stderr": 0.057598182022821603,
        "coqa_em_stderr": 0.06346075617022023
      },
      "benchmarks": {
        "model_size_mb": 965.1290283203125,
        "memory_allocated_mb": 1006.41845703125,
        "memory_peak_mb": 1026.22802734375,
        "prefill_latency_ms": {
          "128": 33.002799600001254,
          "256": 32.78622339999515,
          "512": 32.89075360000311,
          "1024": 34.01330160000384
        },
        "decode_latency_ms_per_token": 25.061116835937547,
        "throughput_tokens_per_sec": {
          "1": 39.686045021204635,
          "4": 102.361355458982,
          "8": 205.114437420158
        },
        "device": "NVIDIA A100-SXM4-40GB"
      }
    },
    {
      "experiment_name": "bnb_4bit_fp4",
      "status": "success",
      "model_size_mb": 965.1290283203125,
      "coqa_metrics": {
        "coqa_f1": 0.5865191613941613,
        "coqa_em": 0.45000000000000007,
        "coqa_f1_stderr": 0.05956074447922055,
        "coqa_em_stderr": 0.06481265518483904
      },
      "benchmarks": {
        "model_size_mb": 965.1290283203125,
        "memory_allocated_mb": 1006.41845703125,
        "memory_peak_mb": 1026.22802734375,
        "prefill_latency_ms": {
          "128": 33.67225760000565,
          "256": 32.832526800007145,
          "512": 32.85028239998837,
          "1024": 33.92540879999615
        },
        "decode_latency_ms_per_token": 24.815546385937548,
        "throughput_tokens_per_sec": {
          "1": 40.14180079981993,
          "4": 102.70688434472686,
          "8": 203.1159844672884
        },
        "device": "NVIDIA A100-SXM4-40GB"
      }
    },
    {
      "experiment_name": "gptq_4bit_g128",
      "status": "error",
      "error": "Dataset scripts are no longer supported, but found c4.py"
    },
    {
      "experiment_name": "awq_4bit_g128",
      "status": "error",
      "error": "The quantization method QuantizationMethod.AWQ does require the model to be pre-quantized. You explicitly passed `pre_quantized=False` meaning your model weights are not quantized. Make sure to pass `pre_quantized=True` while knowing what you are doing."
    }
  ],
  "limit": 50,
  "run_type": "comparison",
  "timestamp": "2026-01-18T19:27:36.853945",
  "saved_to": "/home/orko/Desktop/McGIll_interviews/llama_rewrite_2/results/results3.json"
}
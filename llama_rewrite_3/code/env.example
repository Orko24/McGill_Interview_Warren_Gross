# =============================================================================
# Environment Variables for Llama 3.2-1B Quantization
# =============================================================================
#
# SETUP:
#   1. Copy this file:  cp env.example .env
#   2. Fill in your HF_TOKEN below
#   3. Run: docker-compose build
#
# =============================================================================

# -----------------------------------------------------------------------------
# REQUIRED: HuggingFace Token
# -----------------------------------------------------------------------------
# Llama 3.2 is a gated model. You MUST:
#   1. Go to: https://huggingface.co/meta-llama/Llama-3.2-1B
#   2. Click "Access repository" and accept Meta's license
#   3. Go to: https://huggingface.co/settings/tokens
#   4. Create a token (read access is enough)
#   5. Paste it below (replace the placeholder)
#
HF_TOKEN=


# -----------------------------------------------------------------------------
# OPTIONAL: GPU Selection (if you have multiple GPUs)
# -----------------------------------------------------------------------------
# Uncomment and set to use a specific GPU
# CUDA_VISIBLE_DEVICES=0


# -----------------------------------------------------------------------------
# OPTIONAL: Weights & Biases (for experiment tracking)
# -----------------------------------------------------------------------------
# If you want to log experiments to W&B, uncomment and add your key
# WANDB_API_KEY=your_wandb_api_key_here


# -----------------------------------------------------------------------------
# OPTIONAL: Modal (if using serverless GPU instead of Docker)
# -----------------------------------------------------------------------------
# Modal token is auto-managed by `modal setup` command
# No need to add anything here for Modal

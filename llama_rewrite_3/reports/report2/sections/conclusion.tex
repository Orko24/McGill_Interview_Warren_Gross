% ============================================================================
% CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We minimized Llama 3.2-1B's bit-width while maximizing CoQA accuracy. Our main findings: (1) 4-bit NF4 achieves 2.44$\times$ memory compression without accuracy loss; (2) NF4 outperforms FP4 by 15\% at identical memory due to distribution-aware quantization; (3) the throughput tradeoff (48\% reduction) is acceptable for memory-constrained deployment.

Code and configurations are available in the accompanying repository.

\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Askari-Hemmat et~al.(2022)]{askari2022injected}
Mohammad~Hossein Askari-Hemmat et~al.
\newblock Qreg: On regularization effects of quantization.
\newblock \emph{arXiv preprint arXiv:2206.12372}, 2022.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llmint8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Frantar and Alistarh(2023)]{frantar2023sparsegpt}
Elias Frantar and Dan Alistarh.
\newblock Sparsegpt: Massive language models can be accurately pruned in
  one-shot.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2023gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, et~al.]{gao2023framework}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, et~al.
\newblock A framework for few-shot language model evaluation.
\newblock \url{https://github.com/EleutherAI/lm-evaluation-harness}, 2023.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou,
  et~al.]{hendrycks2021mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, et~al.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and
  Matias]{leviathan2023speculative}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Li et~al.(2020)Li, Dong, and Wang]{li2019apot}
Yuhang Li, Xin Dong, and Wei Wang.
\newblock Additive powers-of-two quantization: An efficient non-uniform
  discretization for neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Lin et~al.(2024)Lin, Tang, Tang, Yang, et~al.]{lin2024awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, et~al.
\newblock Awq: Activation-aware weight quantization for llm compression and
  acceleration.
\newblock In \emph{Conference on Machine Learning and Systems (MLSys)}, 2024.

\bibitem[Lloyd(1982)]{lloyd1982least}
Stuart Lloyd.
\newblock Least squares quantization in pcm.
\newblock \emph{IEEE Transactions on Information Theory}, 28\penalty0
  (2):\penalty0 129--137, 1982.

\bibitem[Max(1960)]{max1960quantizing}
Joel Max.
\newblock Quantizing for minimum distortion.
\newblock \emph{IRE Transactions on Information Theory}, 6\penalty0
  (1):\penalty0 7--12, 1960.

\bibitem[{Meta}(2024)]{meta2024llama32}
{Meta}.
\newblock Llama 3.2-1b.
\newblock \url{https://huggingface.co/meta-llama/Llama-3.2-1B}, 2024.
\newblock Accessed: 2026-01-19.

\bibitem[Ouyang et~al.(2025)]{ouyang2025lowbit}
Shuo Ouyang et~al.
\newblock Low-bit quantization favors undertrained llms.
\newblock In \emph{Proceedings of the 63rd Annual Meeting of the Association
  for Computational Linguistics (ACL)}, 2025.

\bibitem[Reddy et~al.(2019)Reddy, Chen, and Manning]{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock Coqa: A conversational question answering challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 249--266, 2019.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics (ACL)}, 2019.

\bibitem[Zhang et~al.(2020)]{zhang2020sqwa}
Jun Zhang et~al.
\newblock Sqwa: Stochastic quantized weight averaging for improving the
  generalization capability of low-precision deep neural networks.
\newblock \emph{arXiv preprint arXiv:2002.00343}, 2020.

\end{thebibliography}

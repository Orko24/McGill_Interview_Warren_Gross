% === PRIMARY SOURCES (Used in main text) ===

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{askari2022injected,
  title={QReg: On Regularization Effects of Quantization},
  author={Askari-Hemmat, Mohammad Hossein and others},
  journal={arXiv preprint arXiv:2206.12372},
  year={2022}
}

@misc{meta2024llama32,
  title={Llama 3.2-1B},
  author={{Meta}},
  year={2024},
  howpublished={\url{https://huggingface.co/meta-llama/Llama-3.2-1B}},
  note={Accessed: 2026-01-19}
}

@article{reddy2019coqa,
  title={CoQA: A Conversational Question Answering Challenge},
  author={Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={249--266},
  year={2019}
}

@misc{gao2023framework,
  title={A framework for few-shot language model evaluation},
  author={Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and others},
  year={2023},
  publisher={Zenodo},
  howpublished={\url{https://github.com/EleutherAI/lm-evaluation-harness}}
}

% === QUANTIZATION METHODS ===

@inproceedings{dettmers2022llmint8,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@inproceedings{frantar2023gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023}
}

@inproceedings{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and others},
  booktitle={Conference on Machine Learning and Systems (MLSys)},
  year={2024}
}

% === INFORMATION THEORY / QUANTIZATION THEORY ===

@article{lloyd1982least,
  title={Least squares quantization in PCM},
  author={Lloyd, Stuart},
  journal={IEEE Transactions on Information Theory},
  volume={28},
  number={2},
  pages={129--137},
  year={1982}
}

@article{max1960quantizing,
  title={Quantizing for minimum distortion},
  author={Max, Joel},
  journal={IRE Transactions on Information Theory},
  volume={6},
  number={1},
  pages={7--12},
  year={1960}
}

@inproceedings{li2019apot,
  title={Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks},
  author={Li, Yuhang and Dong, Xin and Wang, Wei},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{zhang2020sqwa,
  title={SQWA: Stochastic Quantized Weight Averaging for Improving the Generalization Capability of Low-Precision Deep Neural Networks},
  author={Zhang, Jun and others},
  journal={arXiv preprint arXiv:2002.00343},
  year={2020}
}

@inproceedings{ouyang2025lowbit,
  title={Low-Bit Quantization Favors Undertrained LLMs},
  author={Ouyang, Shuo and others},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2025}
}

% === LLM MODELS ===

@article{touvron2023llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

% === BENCHMARKS ===

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2016}
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@inproceedings{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

% === MODEL COMPRESSION (OTHER) ===

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{leviathan2023speculative,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2023}
}

# Design Choices - Why We Did What We Did

## Overview

This document explains every major design decision in plain English.

---

## 1. Why Modal Instead of Local/Docker?

### The Problem
- User has no local GPU (only 13GB RAM, AMD Ryzen CPU)
- Docker alone can't provide GPU access
- Need cloud GPU for experiments

### The Solution
**Modal** - A serverless GPU platform that:
- Provisions GPUs on-demand
- Charges by the second (~$0.50-1/hour for A10G)
- Handles all the Docker/CUDA complexity automatically

### Trade-off
| Option | Pros | Cons |
|--------|------|------|
| Local CPU | Free | Too slow, not enough RAM |
| Colab | Free tier | Session timeouts, limited |
| **Modal** âœ“ | Fast, easy, reliable | Costs ~$3-5 for full experiments |
| Lambda/AWS | More control | Complex setup |

---

## 2. Why BitsAndBytes Over GPTQ/AWQ?

### The Contenders

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QUANTIZATION METHODS COMPARISON                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  BitsAndBytes (BnB)                                                        â”‚
â”‚  â”œâ”€â”€ Setup: pip install bitsandbytes âœ“ Easy                               â”‚
â”‚  â”œâ”€â”€ Usage: Just add quantization_config to from_pretrained()             â”‚
â”‚  â”œâ”€â”€ Calibration: NOT REQUIRED âœ“                                          â”‚
â”‚  â””â”€â”€ Quality: Good (NF4 especially)                                        â”‚
â”‚                                                                             â”‚
â”‚  GPTQ                                                                       â”‚
â”‚  â”œâ”€â”€ Setup: pip install optimum auto-gptq                                  â”‚
â”‚  â”œâ”€â”€ Usage: Need calibration data + quantization step                     â”‚
â”‚  â”œâ”€â”€ Calibration: REQUIRED (128+ samples recommended)                     â”‚
â”‚  â””â”€â”€ Quality: Often slightly better than BnB                              â”‚
â”‚                                                                             â”‚
â”‚  AWQ                                                                        â”‚
â”‚  â”œâ”€â”€ Setup: pip install autoawq                                            â”‚
â”‚  â”œâ”€â”€ Usage: Need calibration data + quantization step                     â”‚
â”‚  â”œâ”€â”€ Calibration: REQUIRED                                                â”‚
â”‚  â””â”€â”€ Quality: Often best, but newer                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why We Chose BitsAndBytes
1. **Simplest setup** - No calibration data needed
2. **Well-tested** - Widely used in the community
3. **Good enough** - NF4 achieves excellent results
4. **Assignment scope** - Focus on systematic comparison, not every method

### What We'd Add Given More Time
- GPTQ with different group sizes (32, 64, 128)
- AWQ for comparison
- 8-bit (if CUDA bug is fixed)

---

## 3. Why NF4 Over FP4?

### The Experiment
We tested both at identical memory usage (965 MB):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NF4 vs FP4 Results                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  NF4 (Normal Float 4):     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  67.6% F1    â”‚
â”‚  FP4 (Floating Point 4):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          58.1% F1    â”‚
â”‚                                                               â”‚
â”‚  Difference: +9.5% F1 (16.4% relative improvement!)          â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why NF4 Wins

**The Math (simplified):**

Neural network weights follow a bell curve (normal distribution):
```
                          â–²
                         â•±â”‚â•²
                        â•± â”‚ â•²
                       â•±  â”‚  â•²
                      â•±   â”‚   â•²
                     â•±    â”‚    â•²
          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          -3Ïƒ     -1Ïƒ    0    +1Ïƒ     +3Ïƒ
          (rare)  (common)    (common) (rare)
```

- **FP4**: Distributes its 16 values evenly across the range
- **NF4**: Packs more values near zero (where weights cluster)

**Result**: NF4 preserves more information where it matters most.

---

## 4. Why Double Quantization = Free Compression

### What Double Quantization Does

```
WITHOUT DOUBLE QUANT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Weights (4-bit): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (compressed)           â”‚
â”‚  Scales (FP32):   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚                   â–²                                         â”‚
â”‚                   â””â”€â”€ These are BIG! 32 bits each          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WITH DOUBLE QUANT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Weights (4-bit):    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (compressed)        â”‚
â”‚  Scales (8-bit):     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          (also compressed!)  â”‚
â”‚  Scale-scales (FP32): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚
â”‚                       â–²                                     â”‚
â”‚                       â””â”€â”€ Only a few of these now          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Our Finding
- With double quant: 67.58% F1
- Without double quant: 67.58% F1
- **Same accuracy, smaller footprint!**

### Recommendation
Always enable double quantization - it's free compression.

---

## 5. Why We Skip 8-bit Quantization

### The Bug

```
Error invalid configuration argument at line 380 in file /src/csrc/ops.cu
```

### What Happened
The BitsAndBytes 8-bit CUDA kernel has a bug on A10G GPUs. This is a known issue with certain GPU architectures.

### What We Tried
1. Added `llm_int8_threshold=6.0`
2. Set `llm_int8_has_fp16_weight=False`
3. Used NVIDIA CUDA 12.1 base image

### Result
Bug persisted. Not worth debugging further for this assignment since 4-bit is more interesting anyway (more aggressive compression).

---

## 6. Why SDPA (Flash Attention)?

### What It Is
SDPA = Scaled Dot-Product Attention, PyTorch's optimized attention implementation.

```python
# We enable this with:
attn_implementation="sdpa"
```

### Benefits
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Standard Attention           â”‚  SDPA / Flash Attention       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                               â”‚                                â”‚
â”‚  Memory: O(nÂ²)                â”‚  Memory: O(n)                 â”‚
â”‚  Speed:  Baseline             â”‚  Speed:  2-4x faster          â”‚
â”‚                               â”‚                                â”‚
â”‚  For 1024 tokens:             â”‚  For 1024 tokens:             â”‚
â”‚  ~4GB memory                  â”‚  ~100MB memory                â”‚
â”‚                               â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why We Use It
- **Free speedup** - Just a config flag
- **Lower memory** - Can fit longer sequences
- **PyTorch native** - No extra dependencies

---

## 7. Why Incremental Saves?

### The Problem
Cloud GPU time costs money. If an experiment crashes, we lose all data.

### The Solution
Save results after EACH experiment, not just at the end:

```python
for exp_name in experiments:
    # Run experiment
    result = run_experiment(exp_name)
    all_results.append(result)
    
    # SAVE IMMEDIATELY
    with open("results.json", "w") as f:
        json.dump(all_results, f)
    print(f"ðŸ’¾ Saved {len(all_results)}/{len(experiments)}")
```

### Benefits
- If experiment 4/6 crashes, we still have results 1-3
- Can monitor progress in real-time
- No wasted compute credits

---

## 8. Why Fail-Fast?

### The Problem
If one experiment fails, running more will likely fail too (same bug).

### The Solution
Stop immediately on first error:

```python
try:
    result = run_experiment(exp_name)
except Exception as e:
    print(f"âŒ FAILED: {e}")
    print("ðŸ›‘ STOPPING to save compute credits")
    return partial_results  # Return what we have
```

### Benefits
- Saves money (cloud GPU = $$$/hour)
- Faster debugging (see error immediately)
- Still get partial results

---

## 9. Why These Specific Hyperparameters?

### Ablation Study Design

We tested **one variable at a time** (scientific method):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ABLATION STUDY: Testing Each Variable Independently                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Baseline: NF4 + double_quant=True + FP16                                  â”‚
â”‚                                                                             â”‚
â”‚  Test 1: Change QUANT TYPE only                                            â”‚
â”‚  â”œâ”€â”€ NF4 â†’ 67.58% F1                                                       â”‚
â”‚  â””â”€â”€ FP4 â†’ 58.07% F1  â† Quant type matters A LOT                          â”‚
â”‚                                                                             â”‚
â”‚  Test 2: Change DOUBLE QUANT only                                          â”‚
â”‚  â”œâ”€â”€ double_quant=True  â†’ 67.58% F1                                        â”‚
â”‚  â””â”€â”€ double_quant=False â†’ 67.58% F1  â† No difference                      â”‚
â”‚                                                                             â”‚
â”‚  Test 3: Change COMPUTE DTYPE only                                         â”‚
â”‚  â”œâ”€â”€ FP16 â†’ 67.58% F1                                                      â”‚
â”‚  â””â”€â”€ BF16 â†’ 67.58% F1  â† No difference                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Matters
- **Quant type**: Most important choice (NF4 >> FP4)
- **Double quant**: Always enable (free)
- **Compute dtype**: Doesn't matter for accuracy (use hardware preference)

---

## 10. Project Structure Choice

### Why This Layout?

```
llama-quantization/
â”œâ”€â”€ config.py        # ALL settings in one place (easy to find/change)
â”œâ”€â”€ quantize.py      # Model loading only (single responsibility)
â”œâ”€â”€ evaluate.py      # Evaluation only (single responsibility)
â”œâ”€â”€ benchmark.py     # Performance measurement only (single responsibility)
â”œâ”€â”€ modal_app.py     # Orchestration + CLI (ties everything together)
â”œâ”€â”€ results/         # All outputs (easy to find)
â””â”€â”€ design_report/   # Documentation (separate from code)
```

### Design Principles
1. **Single Responsibility**: Each file does ONE thing
2. **Config Centralization**: All settings in `config.py`
3. **Separation of Concerns**: Load â†’ Evaluate â†’ Benchmark â†’ Save
4. **Easy Navigation**: Logical folder structure

### Benefits
- Easy to understand (read one file at a time)
- Easy to modify (change one thing in one place)
- Easy to test (each module is independent)
- Easy to reproduce (config saved with results)



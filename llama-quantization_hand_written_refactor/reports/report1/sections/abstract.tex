% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We investigate post-training quantization techniques for the Llama 3.2-1B language model, systematically evaluating the trade-off between model compression and task accuracy on the Conversational Question Answering (CoQA) benchmark. Through extensive ablation studies comparing 4-bit quantization formats (NF4 vs. FP4), double quantization, and compute precision (FP16 vs. BF16), we find that \textbf{4-bit NF4 quantization achieves 2.44$\times$ memory compression with no accuracy degradation}. Notably, NF4 outperforms FP4 by 9.5\% F1 at identical memory cost, demonstrating that quantization format selection is critical. Double quantization provides additional compression at no accuracy cost. Our modular codebase enables reproducible experimentation and is publicly available.
\end{abstract}


